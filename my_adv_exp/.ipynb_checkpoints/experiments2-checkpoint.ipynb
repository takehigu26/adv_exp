{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_across import get_original_model, get_modified_model\n",
    "from datasets import get_german, prep_data\n",
    "from evaluate import my_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "class BaseModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        X, y = data\n",
    "        with tf.GradientTape(persistent=True) as t:\n",
    "            y_pred = self(X, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        grads = t.gradient(loss, self.trainable_variables)\n",
    "        del t\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xts, ytr, yts = get_german()\n",
    "X_test, X_train, _, y_train = prep_data(Xtr, Xts, ytr, yts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=Xtr.shape[1])\n",
    "num_hidden = 100\n",
    "layer1 = tf.keras.layers.Dense(num_hidden, activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l2(0.03))(inputs)\n",
    "layer2 = tf.keras.layers.Dense(num_hidden, activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l2(0.03))(layer1)\n",
    "layer3 = tf.keras.layers.Dense(num_hidden, activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l2(0.03))(layer2)\n",
    "outputs = tf.keras.layers.Dense(2, activation=tf.nn.softmax)(layer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel(inputs, outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.01), loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f32345bd910>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def step_decay(epoch):\n",
    "    if epoch < 900: return 0.01\n",
    "    else: return 0.001\n",
    "lr_decay = LearningRateScheduler(step_decay)\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "            batch_size=200,\n",
    "            epochs=1000,\n",
    "            callbacks = [lr_decay],\n",
    "            verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.815"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_accuracy_score(yts, model(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = \n",
    "alpha = 0.\n",
    "class AdversarialModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        X, y = data\n",
    "        with tf.GradientTape(persistent=True) as t:\n",
    "            t.watch(X)\n",
    "            y_pred = self(X, training=True)\n",
    "            performance_loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "            explanation_loss_entire = t.gradient(performance_loss, X)\n",
    "            explanation_loss = tf.Variable(0.)\n",
    "            for target in targets:\n",
    "                explanation_loss_r = explanation_loss_entire[:, target[0]]\n",
    "                explanation_loss_r = tf.norm(explanation_loss_r, 1)\n",
    "                explanation_loss_r = explanation_loss_r * target[1]\n",
    "                explanation_loss.assign_add(explanation_loss_r)\n",
    "            total_loss = perf_loss + (alpha * exp_loss / X.shape[0])\n",
    "            grads = t.gradient(total_loss, self.trainable_variables)\n",
    "        del t\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
